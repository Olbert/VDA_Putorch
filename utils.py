# -*- coding: utf-8 -*-
"""lab_original.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zPEiXOQCj8RywxhNJxoUrXOU0nw12Kb2
"""

import nibabel
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook
import seaborn as sns
import os
import random
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import os
import h5py

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
np.random.seed(0)
random.seed(0)

"""# Loading Brats dataset"""


class DataLoader():
    @staticmethod
    def load_3d_volume_as_array(filename):
        if '.nii' in filename:
            return DataLoader.load_nifty_volume_as_array(filename)
        elif '.mha' in filename:
            return DataLoader.load_mha_volume_as_array(filename)

    @staticmethod
    def load_mha_volume_as_array(filename):
        img = sitk.ReadImage(filename)
        nda = sitk.GetArrayFromImage(img)
        return nda

    @staticmethod
    def load_nifty_volume_as_array(filename, with_header=False):
        img = nibabel.load(filename)
        data = img.get_data()
        data = np.transpose(data, [2, 1, 0])
        if (with_header):
            return data, img.affine, img.header
        else:
            return data

    @staticmethod
    def download_data(folders, threshold=5000000, data="healthy", mode="full", save_method="npy"):
        train_imgs = []
        count = 0
        k = 0
        imgs = []
        for patient_path in folders:

            modalities = os.listdir(patient_path)

            seg_img_name = [img for img in modalities if "seg.nii" in img][0]
            seg_img = DataLoader.load_3d_volume_as_array(os.path.join(patient_path, seg_img_name))

            num_layers = seg_img.shape[0]
            seg_lyr_sums = [int(np.sum(img)) for img in seg_img]

            modlt_imgs = []
            for train_modlt in sorted(list(set(modalities) - set([seg_img_name]))):
                print(train_modlt)
                modlt_imgs.append(DataLoader.load_3d_volume_as_array(os.path.join(patient_path, train_modlt)))
            for xth_layer in range(int(num_layers * 0.25), int(num_layers * 0.75)):

                # only grab image slices without labeled cancer
                if (data == "healthy"):
                    if seg_lyr_sums[xth_layer] == 0:  # If sum of damaged cells == 0
                        img_mod_list = []
                        ample_info_check = False
                        for modlt_img in modlt_imgs:
                            img_mod_list.append(modlt_img[xth_layer])
                            if (np.sum(modlt_img[xth_layer])) > 5000000:
                                ample_info_check = True
                                count += 1
                        if (ample_info_check):
                            train_imgs.append(img_mod_list)

                elif data == "unhealthy":
                    if seg_lyr_sums[xth_layer] > 8000:
                        img_mod_list = []
                        ample_info_check = False
                        for modlt_img in modlt_imgs:
                            img_mod_list.append(modlt_img[xth_layer])
                            if (np.sum(modlt_img[xth_layer])) > 5000000:
                                ample_info_check = True
                                count += 1
                        if (ample_info_check):
                            train_imgs.append(img_mod_list)
                else:
                    print("No such data")

        if save_method == "npy":
            np.save("./dataset_" + data + ".npy", train_imgs)

        elif save_method == "hdf5":
            try:
                hdf5_dataset = h5py.File("./dataset_" + data, "w")
                dset = hdf5_dataset.create_dataset('train_imgs', data=train_imgs)
            except:
                hdf5_dataset = h5py.File("./dataset_" + data, "a")
                del hdf5_dataset['train_imgs']
                hdf5_dataset.create_dataset("train_imgs", data=train_imgs)
                print('File already exists. Overwriting...')
            hdf5_dataset.close()
        return train_imgs

    def get_small_dataset(self, folders, number=10, save_method="npy"):
        healthy = []
        unhealthy = []
        mask = []
        count = 0
        current_healthy = 0
        current_unhealthy = 0
        stop = False
        for patient_path in folders:
            if not stop:
                modalities = os.listdir(patient_path)

                seg_img_name = [img for img in modalities if "seg.nii" in img][0]
                seg_img = DataLoader.load_3d_volume_as_array(os.path.join(patient_path, seg_img_name))

                num_layers = seg_img.shape[0]
                seg_lyr_sums = [int(np.sum(img)) for img in seg_img]

                modlt_imgs = []
                for train_modlt in sorted(list(set(modalities) - set([seg_img_name]))):
                    print(train_modlt)
                    modlt_imgs.append(DataLoader.load_3d_volume_as_array(os.path.join(patient_path, train_modlt)))
                for xth_layer in range(int(num_layers * 0.25), int(num_layers * 0.75)):
                    if (current_healthy < number) | (current_unhealthy < number):
                        if seg_lyr_sums[xth_layer] == 0:  # If sum of damaged cells == 0
                            if current_healthy < number:
                                img_mod_list = []
                                ample_info_check = False
                                for modlt_img in modlt_imgs:
                                    img_mod_list.append(modlt_img[xth_layer])
                                    if (np.sum(modlt_img[xth_layer])) > 5000000:
                                        ample_info_check = True
                                        count += 1
                                if (ample_info_check):
                                    healthy.append(img_mod_list)
                                current_healthy += 1

                        elif seg_lyr_sums[xth_layer] > 8000:
                            if current_unhealthy < number:
                                img_mod_list = []
                                ample_info_check = False
                                for modlt_img in modlt_imgs:
                                    img_mod_list.append(modlt_img[xth_layer])
                                    if (np.sum(modlt_img[xth_layer])) > 5000000:
                                        ample_info_check = True
                                        count += 1
                                if (ample_info_check):
                                    unhealthy.append(img_mod_list)
                                mask.append(seg_img[xth_layer])

                                current_unhealthy += 1
                    else:
                        stop = True

        if save_method == "npy":
            np.save("./dataset_" + str(number) + "healthy.npy", healthy)
            np.save("./dataset_" + str(number) + "unhealthy.npy", unhealthy)
            np.save("./dataset_" + str(number) + "mask.npy", mask)

        elif save_method == "hdf5":
            try:
                hdf5_dataset = h5py.File("./dataset_" + str(number), "w")
                dset = hdf5_dataset.create_dataset('healthy', data=healthy)
                dset = hdf5_dataset.create_dataset('unhealthy', data=unhealthy)
                dset = hdf5_dataset.create_dataset('mask', data=mask)
            except:
                hdf5_dataset = h5py.File("./dataset_" + str(number), "a")
                del hdf5_dataset['healthy']
                del hdf5_dataset['unhealthy']
                del hdf5_dataset['mask']
                dset = hdf5_dataset.create_dataset('healthy', data=healthy)
                dset = hdf5_dataset.create_dataset('unhealthy', data=unhealthy)
                dset = hdf5_dataset.create_dataset('mask', data=mask)
                print('File already exists. Overwriting...')
            hdf5_dataset.close()

        return healthy, unhealthy, mask

    def get_test_image(self, folders, number=10, save_method="npy"):
        healthy = []
        unhealthy = []
        mask = []
        count = 0
        current_healthy = 0
        current_unhealthy = 0
        stop = False
        cc = 0
        for patient_path in folders:
            if cc < 100:
                cc+=1
                continue
            if not stop:
                modalities = os.listdir(patient_path)

                seg_img_name = [img for img in modalities if "seg.nii" in img][0]
                seg_img = DataLoader.load_3d_volume_as_array(os.path.join(patient_path, seg_img_name))

                num_layers = seg_img.shape[0]
                seg_lyr_sums = [int(np.sum(img)) for img in seg_img]

                modlt_imgs = []
                for train_modlt in sorted(list(set(modalities) - set([seg_img_name]))):
                    print(train_modlt)
                    modlt_imgs.append(DataLoader.load_3d_volume_as_array(os.path.join(patient_path, train_modlt)))

                for xth_layer in range(int(num_layers * 0.25), int(num_layers * 0.75)):
                    if (current_healthy < number) | (current_unhealthy < number):
                        if seg_lyr_sums[xth_layer] > 8000:
                            if current_unhealthy < number:
                                img_mod_list = []
                                ample_info_check = False
                                for modlt_img in modlt_imgs:
                                    img_mod_list.append(modlt_img[xth_layer])
                                    if (np.sum(modlt_img[xth_layer])) > 5000000:
                                        ample_info_check = True
                                        count += 1
                                if (ample_info_check):
                                    unhealthy.append(img_mod_list)
                                mask.append(seg_img[xth_layer])

                                current_unhealthy += 1
                    else:
                        stop = True

        if save_method == "npy":
            np.save("./dataset_" + str(number) + "unhealthy.npy", unhealthy)
            np.save("./dataset_" + str(number) + "mask.npy", mask)

        elif save_method == "hdf5":
            try:
                hdf5_dataset = h5py.File("./dataset_" + str(number), "w")
                dset = hdf5_dataset.create_dataset('unhealthy', data=unhealthy)
                dset = hdf5_dataset.create_dataset('mask', data=mask)
            except:
                hdf5_dataset = h5py.File("./dataset_" + str(number), "a")
                del hdf5_dataset['unhealthy']
                del hdf5_dataset['mask']
                dset = hdf5_dataset.create_dataset('unhealthy', data=unhealthy)
                dset = hdf5_dataset.create_dataset('mask', data=mask)
                print('File already exists. Overwriting...')
            hdf5_dataset.close()

        return healthy, unhealthy, mask

class PreProcessor():
    """# Loading Training Data (Preprocessing)"""

    @staticmethod
    def augment(data, percentage=1, type="square", size=(15,50)):
        length = int(data.shape[0] * percentage)
        if type=="square":
            for i in range(0, length):
                cur_size = int(size[0]+ np.random.rand(1) * (size[1]-size[0]))
                x = int(np.random.rand(1) * (data.shape[2] - cur_size))
                y = int(np.random.rand(1) * (data.shape[3] - cur_size))
                for x_id in range(0, cur_size):
                    for y_id in range(0, cur_size):
                        for mod in range(0, 3):
                            data[i, mod, x + x_id, y + y_id] = 1

        return data

    @staticmethod
    def show():
        train_imgs = np.load("./dataset.npy")
        print(train_imgs.shape)

        train_imgs = np.array(train_imgs)
        train_imgs = train_imgs.reshape(-1, 4 * 240 * 240)
        print(train_imgs.shape)
        scaler = MinMaxScaler()
        scaler.fit(train_imgs)
        train_imgs = scaler.transform(train_imgs)
        train_imgs = np.reshape(train_imgs, (-1, 4, 240, 240))
        print(train_imgs.shape)

        f1 = plt.figure(figsize=(12, 12))
        ax1 = f1.add_subplot(221)
        ax2 = f1.add_subplot(222)
        ax3 = f1.add_subplot(223)
        ax4 = f1.add_subplot(224)
        num = 10
        ax1.imshow(train_imgs[num][0], cmap="gray")
        ax2.imshow(train_imgs[num][1], cmap="gray")
        ax3.imshow(train_imgs[num][2], cmap="gray")
        ax4.imshow(train_imgs[num][3], cmap="gray")

    """# Splitting Data into training Validation and Test set"""

    @staticmethod
    def split(train_imgs, save=False, folder="./"):
        train_data, test_vald_data = train_test_split(train_imgs, test_size=0.20, random_state=100)
        valid_data, test_data = train_test_split(test_vald_data, test_size=0.30, random_state=100)
        train_data = np.array((train_data))
        valid_data = np.array((valid_data))
        test_data = np.array((test_data))
        print(train_data.shape)
        print(valid_data.shape)
        print(test_data.shape)

        # see the percentage of train validation and testdata
        print((train_data.shape[0] + valid_data.shape[0] + test_data.shape[0]))
        print(train_data.shape[0] / (train_data.shape[0] + valid_data.shape[0] + test_data.shape[0]))
        print(valid_data.shape[0] / (train_data.shape[0] + valid_data.shape[0] + test_data.shape[0]))
        print(test_data.shape[0] / (train_data.shape[0] + valid_data.shape[0] + test_data.shape[0]))
        if save:
            np.save(folder + "dataset_split.npy", np.array((train_data, valid_data, test_data)))
        return train_data, valid_data, test_data

    """#To use One image only"""

    # if training only on one image, model selecting for a network with minimum bias
    # train_imgs = [train_data[30]]
    # train_imgs = np.asarray(train_imgs)
    # train_imgs.shape

    # plt.imshow(np.reshape(train_data[5][3], (240, 240)), cmap='gray')
    #
    # plt.imshow(np.reshape(train_data[5][3], (240, 240)))
